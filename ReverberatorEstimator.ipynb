{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af835bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 14:04:51.229535: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:51.229577: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import tensorflow_io as tfio\n",
    "import functools\n",
    "from pedalboard import load_plugin\n",
    "from ReverberatorEstimator import layers, loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 48000\n",
    "num_params = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7638df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 14:04:53.075528: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2021-12-01 14:04:53.312734: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:53.312862: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:53.312956: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:53.313045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:53.313136: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:53.313226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:53.313315: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:53.313405: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-12-01 14:04:53.313424: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-01 14:04:53.314597: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "target_audio = tfio.audio.AudioIOTensor(\"snap_wet.wav\")\n",
    "target_audio = target_audio.to_tensor()\n",
    "target_audio = tf.slice(target_audio, begin=[0,0], size=[-1,1])\n",
    "target_audio = tf.cast(target_audio, tf.float32) / 32768.0\n",
    "target_audio = tf.squeeze(target_audio)\n",
    "target_audio = tf.reshape(target_audio,(1, 96000))\n",
    "input_audio = tfio.audio.AudioIOTensor(\"snap_dry.wav\")\n",
    "input_audio = input_audio.to_tensor()\n",
    "input_audio = tf.slice(input_audio, begin=[0,0], size=[-1,1])\n",
    "input_audio = tf.cast(input_audio, tf.float32) / 32768.0\n",
    "input_audio = tf.squeeze(input_audio)\n",
    "input_audio = tf.reshape(input_audio,(1, 96000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5126ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n"
     ]
    }
   ],
   "source": [
    "logmelgram = layers.LogMelgramLayer(1024, 256, 128, sample_rate, 0.0, sample_rate//2, 1e-6)\n",
    "audio_time = tfkl.Input(shape=(96000,), name=\"audio_time\")\n",
    "\n",
    "x = logmelgram(audio_time)\n",
    "x = tfkl.BatchNormalization(name=\"input_norm\")(x)\n",
    "encoder_model = tfk.applications.MobileNetV2(input_shape=(x.shape[1], x.shape[2], x.shape[3]), alpha=1.0,\n",
    "                                            include_top=True, weights=None, input_tensor=None, pooling=None,\n",
    "                                            classes=np.sum(num_params).item(), classifier_activation=\"sigmoid\")\n",
    "\n",
    "hidden = encoder_model(x)\n",
    "\n",
    "parameter_model = tfk.models.Model(audio_time, hidden, name=\"parameter_model\")\n",
    "\n",
    "parameters = parameter_model(audio_time)\n",
    "\n",
    "vstlayer = layers.VSTProcessor(\"../Reverberator.vst3\", sample_rate)\n",
    "output = vstlayer([audio_time, parameters])\n",
    "\n",
    "model = tfk.models.Model(audio_time, output, name=\"full_model\")\n",
    "\n",
    "spectral_loss = loss.multiScaleSpectralLoss(sr=sample_rate)\n",
    "\n",
    "optimizer = tfk.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=spectral_loss, metrics=['mae'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523328db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"parameter_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "audio_time (InputLayer)      [(None, 96000)]           0         \n",
      "_________________________________________________________________\n",
      "log_melgram_layer (LogMelgra (None, 372, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "input_norm (BatchNormalizati (None, 372, 128, 1)       4         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_372 (Functi (None, 28)                2293276   \n",
      "=================================================================\n",
      "Total params: 2,293,280\n",
      "Trainable params: 2,259,166\n",
      "Non-trainable params: 34,114\n",
      "_________________________________________________________________\n",
      "Model: \"full_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "audio_time (InputLayer)         [(None, 96000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "parameter_model (Functional)    (None, 28)           2293280     audio_time[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "vst_processor (VSTProcessor)    None                 0           audio_time[0][0]                 \n",
      "                                                                 parameter_model[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 2,293,280\n",
      "Trainable params: 2,259,166\n",
      "Non-trainable params: 34,114\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "parameter_model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600a6fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 14:04:55.227282: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ./training_checkpoints: Failed precondition: training_checkpoints; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f33cf81d610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore from latest checkpoint\n",
    "model.load_weights(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "207ff530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5000084  0.4999778  0.49988297 0.5000118  0.49991092 0.5001264\n",
      " 0.49997008 0.49988535 0.5000775  0.49988642 0.49998417 0.5000142\n",
      " 0.49986145 0.50010145 0.49993053 0.50002843 0.49997926 0.50013983\n",
      " 0.50003535 0.49998054 0.49981967 0.49988514 0.5001355  0.4999809\n",
      " 0.4999524  0.49993125 0.50000525 0.4999644 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No protocol specified\n",
      "No protocol specified\n"
     ]
    }
   ],
   "source": [
    "audio_pre = (model.call(input_audio)).numpy()[0]\n",
    "old_params = parameter_model(input_audio).numpy()[0]\n",
    "print(old_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2541dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cp = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
    "                                  factor=0.2,\n",
    "                                  patience=10,\n",
    "                                  cooldown=0,\n",
    "                                  verbose=1,\n",
    "                                  mode='min',\n",
    "                                  min_lr=0.0000016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ec16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.2002 - mae: 0.0070\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.19949\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.1984 - mae: 0.0069\n",
      "\n",
      "Epoch 00002: loss improved from 0.19949 to 0.19841, saving model to ./training_checkpoints\n",
      "INFO:tensorflow:Assets written to: ./training_checkpoints/assets\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.1984 - mae: 0.0069\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.19841\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n",
      "No protocol specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.1974 - mae: 0.0069\n",
      "\n",
      "Epoch 00004: loss improved from 0.19841 to 0.19744, saving model to ./training_checkpoints\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history = model.fit(input_audio, target_audio, verbose=1, epochs=100,\n",
    "         callbacks=[model_cp,reduce_lr])\n",
    "print(\"Training took %d seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65516d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_audio = model(input_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(7,10))\n",
    "ax[0].plot(audio_pre)\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_pre)), ref=np.max)\n",
    "img = librosa.display.specshow(D, y_axis='linear', x_axis='time',\n",
    "                               sr=sample_rate, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ab5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "ax[0,0].plot(output_audio.numpy()[0])\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(output_audio.numpy()[0])), ref=np.max)\n",
    "img = librosa.display.specshow(D, y_axis='linear', x_axis='time',\n",
    "                               sr=sample_rate, ax=ax[1,0])\n",
    "ax[0,1].plot(target_audio.numpy()[0])\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(target_audio.numpy()[0])), ref=np.max)\n",
    "img = librosa.display.specshow(D, y_axis='linear', x_axis='time',\n",
    "                               sr=sample_rate, ax=ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff082009",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(output_audio, rate=sample_rate, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio_pre, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b64e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(target_audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89028e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump values to .csv files\n",
    "np.savetxt(\"output_audio.csv\", output_audio.numpy()[0], delimiter=\",\")\n",
    "np.savetxt(\"target_audio.csv\", target_audio.numpy()[0], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bcc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameter_model(input_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58656f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = parameters.numpy()[0]\n",
    "filter_c = 1\n",
    "for i in range(num_params):\n",
    "    if i < 4:\n",
    "        print(\"b_%i = %f\" % (i, params[i]))\n",
    "    elif i < 8:\n",
    "        print(\"c_%i = %f\" % (i-4, params[i]))\n",
    "    else:\n",
    "        j = (i-8) % 5\n",
    "        \n",
    "        if j is 0:\n",
    "            print(\"\\nFilter %i:\" % filter_c)\n",
    "            filter_c = filter_c + 1\n",
    "            print(\"c_hp = %f\" % params[i])\n",
    "        elif j is 1:\n",
    "            print(\"c_bp = %f\" % params[i])\n",
    "        elif j is 2:\n",
    "            print(\"c_lp = %f\" % params[i])\n",
    "        elif j is 3:\n",
    "            print(\"g = %f\" % params[i])\n",
    "        elif j is 4:\n",
    "            print(\"R = %f\" % params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_diff = old_params - params\n",
    "print(param_diff)\n",
    "plt.stem(param_diff)\n",
    "plt.ylim([-1, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
