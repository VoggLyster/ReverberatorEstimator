{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd49c8d8",
   "metadata": {},
   "source": [
    "# ReverberatorEstimator notebook\n",
    "Jupyter Notebook for the Master Thesis work Parametric Tuning of Extended Reverberation Algorithm Using Neural Networks by SÃ¸ren V.K. Lyster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0455f5",
   "metadata": {},
   "source": [
    "## Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af835bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow.keras as tfk\n",
    "from ReverberatorEstimator import loss, models, utils, config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b9496",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0300d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184656f",
   "metadata": {},
   "source": [
    "## Setup variables for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = config.k\n",
    "sample_rate = k['sample_rate']\n",
    "sample_length = k['sample_length']\n",
    "num_epochs = k['epochs']\n",
    "num_processors = k['n_processors']\n",
    "steps_per_epoch = k['steps_per_epoch']\n",
    "batch_size = steps_per_epoch * num_processors\n",
    "epsilon = k['epsilon']\n",
    "learning_rate = k['learning_rate']\n",
    "dry_audio_path = k['dry_audio_path']\n",
    "wet_audio_path = k['wet_audio_path']\n",
    "vst_path = k['vst_path']\n",
    "time_loss_weight = k['time_loss_weight']\n",
    "spectral_loss_weight = k['spectral_loss_weight']\n",
    "envelope_loss_weight = k['envelope_loss_weight']\n",
    "echo_density_loss_weight = k['echo_density_loss_weight']\n",
    "use_multiscale = k['use_multiscale']\n",
    "num_params = k['n_parameters']\n",
    "parameter_map = k['parameter_map']\n",
    "non_trainable_parameters = k['non_trainable_parameters']\n",
    "pretrained_weights = k['pretrained_weights']\n",
    "checkpoint_path = k['checkpoint_path']\n",
    "\n",
    "print(parameter_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220de5f6",
   "metadata": {},
   "source": [
    "## Setup dataset for batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94583cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = utils.get_dataset(dry_audio_path, wet_audio_path, batch_size, resample=True, old_sample_rate=48000, new_sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37e56e",
   "metadata": {},
   "source": [
    "## Create layers, create partial models, and compile full model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5126ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, parameter_model, processor = models.get_models(sample_length, sample_rate, num_params, num_processors, \n",
    "                                                vst_path, epsilon, parameter_map, non_trainable_parameters, \n",
    "                                                pretrained_weights)\n",
    "\n",
    "reverberation_loss = loss.reverberationLoss(sample_rate=sample_rate,\n",
    "    spectral_loss_weight=spectral_loss_weight,\n",
    "    spectral_loss_type='L1',\n",
    "    time_loss_weight=time_loss_weight,\n",
    "    time_loss_type='L1',\n",
    "    envelope_loss_weight=envelope_loss_weight,\n",
    "    envelope_loss_type='L1',\n",
    "    echo_density_weight=echo_density_loss_weight,\n",
    "    echo_density_type='L1',\n",
    "    use_multiscale=use_multiscale,\n",
    "    )                     \n",
    "\n",
    "optimizer = tfk.optimizers.Adam(learning_rate=learning_rate) \n",
    "model.compile(optimizer=optimizer, loss=reverberation_loss, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0dd1f",
   "metadata": {},
   "source": [
    "## Print model summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523328db",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53608d5",
   "metadata": {},
   "source": [
    "## Setup checkpoint and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "model_cp = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                             save_weights_only=True,\n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
    "                              factor=0.5,\n",
    "                              patience=500,\n",
    "                              cooldown=1,\n",
    "                              verbose=1,\n",
    "                              mode='auto',\n",
    "                              min_lr=1e-10)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tfk.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efdf89d",
   "metadata": {},
   "source": [
    "## Restore from previous checkpoint if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print(\"No previous checkpoints found at %s\" % checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608b1db",
   "metadata": {},
   "source": [
    "## Run model and save data before training for analysis and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ff530",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audio = tf.reshape(x_train[0], (1, sample_length))\n",
    "target_audio = tf.reshape(y_train[0], (1, sample_length))\n",
    "\n",
    "audio_pre = (model.call(input_audio)).numpy()[0]\n",
    "old_params = parameter_model(input_audio).numpy()[0]\n",
    "print(old_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e99a27",
   "metadata": {},
   "source": [
    "## Run the model.fit to begin training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18544c22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history = model.fit(x_train, y_train, verbose=1, epochs=num_epochs, steps_per_epoch=steps_per_epoch,\n",
    "         callbacks=[model_cp, lr_callback])\n",
    "print(\"Training took %d seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6c1e8",
   "metadata": {},
   "source": [
    "## Plot training loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,5))\n",
    "ax.plot(history.history['loss'])\n",
    "ax.set_title('loss')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3131f",
   "metadata": {},
   "source": [
    "## Run a forward pass and get the output audio of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "output_audio = model(input_audio)\n",
    "processor.print_current_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377b211",
   "metadata": {},
   "source": [
    "## Display the output audio from before training the model\n",
    "This is done to inspect the changes the training has done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_single(audio_pre, sample_rate, sample_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e183c",
   "metadata": {},
   "source": [
    "## Plot the output audio against the target audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ab5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_output_and_target(output_audio, target_audio, sample_rate)\n",
    "IPython.display.display(IPython.display.Audio(output_audio, rate=sample_rate, autoplay=True))\n",
    "IPython.display.display(IPython.display.Audio(target_audio, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc629b7",
   "metadata": {},
   "source": [
    "## Plot the loss function differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_differences(output_audio, target_audio, sample_rate, weights=[time_loss_weight,spectral_loss_weight,envelope_loss_weight,echo_density_loss_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_differences(output_audio, tf.reshape(tf.convert_to_tensor(audio_pre), (1,sample_length)), sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18c3a4",
   "metadata": {},
   "source": [
    "## Print the parameters\n",
    "These parameters are from the parameter model subpart of the full model. These values are transferable to the FDN reverberator plugin at [https://github.com/VoggLyster/Reverberator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bcc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = parameter_model(input_audio).numpy()[0]\n",
    "print('New parameter set: ', params)\n",
    "plt.stem(params)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abee6ae",
   "metadata": {},
   "source": [
    "## Plot the parameter differences of before and after training\n",
    "This shows the movement of the parameters after training and can give a good picture of the momentum of the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_diff = params - old_params\n",
    "print('Parameter set difference: ', param_diff)\n",
    "plt.stem(param_diff)\n",
    "plt.ylim(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb5a6f",
   "metadata": {},
   "source": [
    "## Generate MUSHRA-ready audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, audio_names = utils.generate_MUSHRA_ready_audio(vst_path, params, sample_rate)\n",
    "for i in range(len(audio_data)):\n",
    "    print(audio_names[i])\n",
    "    IPython.display.display(IPython.display.Audio(audio_data[i], rate=sample_rate))\n",
    "utils.write_audio_files(audio_data, audio_names, 'MUSHRA_audio/Wet/AbletonReverb', sample_rate)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "921f65f54d51df3659d5ca2f2f53fd8912cb489ac368cd30f2116122a83f7a56"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
